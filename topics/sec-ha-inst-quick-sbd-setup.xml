<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<topic xmlns="http://docbook.org/ns/docbook"
 xml:id="sec-ha-inst-quick-sbd-setup">
 <title>Setting Up Softdog Watchdog and SBD</title>
 <!--Taken from ha_storage_protection.xml (pro.ha.storage.protect.watchdog)?-->
 <para> In SUSE Linux Enterprise Server, watchdog support in the kernel is
  enabled by default: It ships with several kernel modules that provide
  hardware-specific watchdog drivers. The High Availability Extension uses the
  SBD daemon as the software component that <quote>feeds</quote> the watchdog. </para>
 <para> The following procedure uses the <systemitem>softdog</systemitem>
  watchdog. </para>
 <important>
  <title>Softdog Limitations</title>
  <para> The softdog driver assumes that at least one CPU is still running. If
   all CPUs are stuck, the code in the softdog driver that should reboot the
   system will never be executed. In contrast, hardware watchdogs keep working
   even if all CPUs are stuck. </para>
  <para>Before using the cluster in a production environment, it is highly
   recommended to replace the <systemitem>softdog</systemitem> module with the
   respective hardware module that best fits your hardware. </para>
  <para>However, if no watchdog matches your hardware, <systemitem
    class="resource">softdog</systemitem> can be used as kernel watchdog
   module.</para>
 </important>
 <procedure xml:id="pro-ha-inst-quick-sbd-setup">
  <step>
   <para> Create a persistent, shared storage as described in <xref
     linkend="sec-ha-inst-quick-sbd-req"/>. </para>
  </step>
  <step>
   <para> Enable the softdog watchdog: </para>
   <!-- See /usr/lib/ha-cluster-functions -->
   <screen><prompt role="root">root # </prompt><command>echo</command> softdog &gt; /etc/modules-load.d/watchdog.conf
<prompt role="root">root # </prompt><command>systemctl</command> restart systemd-modules-load</screen>
  </step>
  <step>
   <!--for the records: <remark>toms 2016-08-09: krig: "The script uses wdctl to check for the
      watchdog, so it should catch the case where the node is created but no
      module is loaded. However, I am not sure it'll be able to spot the case
      where the wrong module is loaded." (ha-devel, 2016-08-08)</remark>-->
   <para>Test if the softdog module is loaded correctly: </para>
   <screen><prompt role="root">root # </prompt><command>lsmod</command> | egrep "(wd|dog)"
softdog                16384  1</screen>
  </step>
  <step>
   <para> On <systemitem class="server">bob</systemitem>, initialize the SBD
    partition: </para>
   <screen><prompt role="root">root # </prompt><command>sbd</command> -d /dev/<replaceable>SBDDEVICE</replaceable> create</screen>
  </step>
  <step>
   <para> Start SBD to listen on the SBD device: </para>
   <screen><prompt role="root">root # </prompt><command>sbd</command> -d /dev/<replaceable>SBDDEVICE</replaceable> watch</screen>
  </step>
  <step>
   <para> On <systemitem class="server">alice</systemitem>, send a test message: </para>
   <screen><prompt role="root">root # </prompt><command>sbd</command> -d /dev/<replaceable>SBDDEVICE</replaceable> message bob test<!--
Sep 22 17:01:00 &node1; sbd: [13412]: info: Received command test from &node1;--></screen>
   <remark>toms 2016-07-22: What to do when the test message fails? How to
    debug?</remark>
  </step>
  <step>
   <para> On <systemitem class="server">bob</systemitem>, check the status with
     <command>systemctl</command> and you should see the received message: </para>
   <screen><prompt role="root">root # </prompt><command>systemctl</command> status sbd
[...]
info: Received command test from alice on disk <replaceable>SBDDEVICE</replaceable></screen>
  </step>
  <step>
   <para> Stop watching the SBD device on <systemitem class="server"
     >bob</systemitem> with: </para>
   <screen><prompt role="root">root # </prompt><command>systemctl</command> stop sbd</screen>
  </step>
 </procedure>
 <para> Testing the SBD fencing mechanism for proper function in case of a split
  brain situation is highly recommended. Such a test can be done by blocking the
  Corosync cluster communication. </para>
</topic>
